---
description: Initial Setup of FastAPI Server and Default Agent Loop
complexity: ⚠️ Medium
---

# 2. FastAPI Gateway and Agent Loop Setup

This plan outlines the steps to build the baseline Python API server using FastAPI, connect it to the Google Gemini SDK, and structure the primary Agentic reasoning loop.

## Objectives
- Initialize the FastAPI router and basic Chat endpoint.
- Configure Pydantic schemas for structured input from the React UI and output back to the UI.
- Implement the core Chat iteration loop (Think -> Answer/Tool) using raw `google-generativeai`.
- Initialize Langfuse telemetry wrappers for observability.

---

## Execution Steps

### Step 1: FastAPI Scaffolding
- Navigate to `src/server/`
- Create `api/main.py`: The FastAPI application entry point. Configure CORS to allow requests from the React development server.
- Create `api/routes/chat.py`: Expose a `POST /api/chat` and a Server-Sent Events (SSE) streaming endpoint `GET /api/chat/stream` for real-time generative responses.
- Create `api/models.py`: Define Pydantic schemas. 
  - `ChatRequest` (prompt, session_id, csharp_context)
  - `ChatResponse` (text_content, action_requests)

### Step 2: Agent Architecture Strategy
- Create `agents/core_agent.py`. This will house the main `while(True)` logic for the AI.
- Define a base system prompt in `agents/prompts.py` establishing the AI's identity as a Manufacturing Engineering Assistant.
- Initialize the `google-generativeai` client, pulling the `GEMINI_API_KEY` from a `.env` file.

### Step 3: Tool Registry Scaffold
- Create `tools/registry.py`.
- Define a dummy tool (e.g., `get_equipment_details(id: str)`) to prove the Gemini function calling loop works natively without LangChain.

### Step 4: Observability Setup
- Ensure `langfuse` import is wrapped around the core agent generation calls to provide visibility into the LLM tokens and latency.

---

## Validation / Testing
1. **API Initialization:** Run `uvicorn api.main:app --reload` from `src/server`. The server must start on port 8000 without crashing.
2. **Schema Validation:** Send an empty JSON POST body to `/api/chat` using `curl`; verify it returns a 422 Unprocessable Entity error defined by Pydantic.
3. **Basic LLM Call:** Provide a `.env` with a Gemini key and send a basic "Hello" prompt to `/api/chat`. The response should correctly return the LLM's text inside the `ChatResponse` JSON structure.
